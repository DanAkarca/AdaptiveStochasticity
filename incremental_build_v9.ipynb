{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNNRahnjev9BfuF+64ptZcT",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DanAkarca/AdaptiveStochasticity/blob/main/incremental_build_v9.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Mutli-network modular system w/ attention-based dropout and communciation delays:\n",
        "- v9 - has a switch of tasks half way through so must change rapidly the dropout."
      ],
      "metadata": {
        "id": "MUkJHJu_JZIY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xtVSD7K8JUdT",
        "outputId": "4e5a9b7f-337f-4625-afbe-cad86dfa8c6c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/60], Loss: 0.0890\n",
            "Epoch [20/60], Loss: 0.0874\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class AttentionBasedAdaptiveDropout(nn.Module):\n",
        "    def __init__(self, initial_rate, min_rate, max_rate, adaptation_rate):\n",
        "        super(AttentionBasedAdaptiveDropout, self).__init__()\n",
        "        self.dropout_rate = nn.Parameter(torch.tensor(initial_rate))\n",
        "        self.min_rate = min_rate\n",
        "        self.max_rate = max_rate\n",
        "        self.adaptation_rate = adaptation_rate\n",
        "\n",
        "    def forward(self, x, attention_weights=None):\n",
        "        if self.training and attention_weights is not None:\n",
        "            mean_attention = attention_weights.mean().item()\n",
        "            self.update_rate(mean_attention)\n",
        "\n",
        "        return F.dropout(x, p=self.dropout_rate.item(), training=self.training)\n",
        "\n",
        "    def update_rate(self, attention_score):\n",
        "        new_rate = self.dropout_rate - self.adaptation_rate * (attention_score - 0.5)\n",
        "        new_rate = torch.clamp(new_rate, self.min_rate, self.max_rate)\n",
        "        self.dropout_rate.data.copy_(new_rate)\n",
        "\n",
        "    def get_current_rate(self):\n",
        "        return self.dropout_rate.item()\n",
        "\n",
        "class AttentionLayer(nn.Module):\n",
        "    def __init__(self, hidden_size, attention_size):\n",
        "        super(AttentionLayer, self).__init__()\n",
        "        self.attention = nn.Sequential(\n",
        "            nn.Linear(hidden_size, attention_size),\n",
        "            nn.Tanh(),\n",
        "            nn.Linear(attention_size, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attention_weights = self.attention(hidden_states)\n",
        "        attention_weights = torch.softmax(attention_weights, dim=1)\n",
        "        return attention_weights\n",
        "\n",
        "class SwappableModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, model_type, dropout_params):\n",
        "        super(SwappableModel, self).__init__()\n",
        "        self.model_type = model_type\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "\n",
        "        if model_type == 'lstm':\n",
        "            self.model = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
        "        elif model_type == 'gru':\n",
        "            self.model = nn.GRU(input_size, hidden_size, batch_first=True)\n",
        "        elif model_type == 'transformer':\n",
        "            self.d_model = ((input_size - 1) // 8 + 1) * 8\n",
        "            self.embedding = nn.Linear(input_size, self.d_model)\n",
        "            self.model = nn.TransformerEncoder(\n",
        "                nn.TransformerEncoderLayer(d_model=self.d_model, nhead=8, batch_first=True), num_layers=2\n",
        "            )\n",
        "        self.fc = nn.Linear(hidden_size if model_type in ['lstm', 'gru'] else self.d_model, output_size)\n",
        "\n",
        "        self.adaptive_dropout = AttentionBasedAdaptiveDropout(**dropout_params)\n",
        "\n",
        "    def forward(self, x, attention_weights=None):\n",
        "        if self.model_type in ['lstm', 'gru']:\n",
        "            out, _ = self.model(x)\n",
        "        else:  # transformer\n",
        "            x = self.embedding(x)\n",
        "            out = self.model(x)\n",
        "\n",
        "        # Apply dropout to each time step\n",
        "        out = self.adaptive_dropout(out, attention_weights)\n",
        "\n",
        "        # Apply final linear layer to each time step\n",
        "        out = self.fc(out)\n",
        "\n",
        "        return out\n",
        "\n",
        "class EnhancedSwappableModelEnsemble(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size, num_secondary, delays, secondary_model_types, primary_model_type, dropout_params):\n",
        "        super(EnhancedSwappableModelEnsemble, self).__init__()\n",
        "        self.input_size = input_size\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.num_secondary = num_secondary\n",
        "        self.delays = delays\n",
        "\n",
        "        # Secondary models\n",
        "        self.secondary_models = nn.ModuleList([\n",
        "            SwappableModel(input_size, hidden_size, output_size, model_type, dropout_params)\n",
        "            for model_type in secondary_model_types\n",
        "        ])\n",
        "\n",
        "        # Attention mechanism\n",
        "        self.attention = AttentionLayer(output_size, hidden_size)\n",
        "\n",
        "        # Primary model (now swappable)\n",
        "        self.primary_model = SwappableModel(input_size + output_size, hidden_size, output_size, primary_model_type, dropout_params)\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size, seq_len, _ = x.size()\n",
        "\n",
        "        # Process through secondary models\n",
        "        secondary_outputs = []\n",
        "        for model, delay in zip(self.secondary_models, self.delays):\n",
        "            out = model(x)\n",
        "            # Apply delay\n",
        "            padding = torch.zeros(batch_size, delay, self.output_size, device=x.device)\n",
        "            delayed_out = torch.cat([padding, out[:, :-delay, :]], dim=1)\n",
        "            secondary_outputs.append(delayed_out.unsqueeze(1))\n",
        "\n",
        "        # Stack secondary outputs\n",
        "        secondary_outputs = torch.cat(secondary_outputs, dim=1)\n",
        "\n",
        "        # Apply attention to secondary outputs\n",
        "        attention_weights = self.attention(secondary_outputs)\n",
        "\n",
        "        # Apply adaptive dropout to each secondary output individually\n",
        "        dropped_secondary_outputs = []\n",
        "        for i, model in enumerate(self.secondary_models):\n",
        "            dropped_out = model.adaptive_dropout(\n",
        "                secondary_outputs[:, i, :, :],\n",
        "                attention_weights[:, i, :].unsqueeze(-1)\n",
        "            )\n",
        "            dropped_secondary_outputs.append(dropped_out.unsqueeze(1))\n",
        "\n",
        "        dropped_secondary_outputs = torch.cat(dropped_secondary_outputs, dim=1)\n",
        "\n",
        "        attended_output = torch.sum(attention_weights * dropped_secondary_outputs, dim=1)\n",
        "\n",
        "        # Combine original input with attended secondary output\n",
        "        combined_input = torch.cat([x, attended_output], dim=2)\n",
        "\n",
        "        # Primary model\n",
        "        out_primary = self.primary_model(combined_input, attention_weights.mean(dim=1))\n",
        "\n",
        "        return out_primary[:, -1, :], attention_weights\n",
        "\n",
        "    def get_dropout_rates(self):\n",
        "        return [model.adaptive_dropout.get_current_rate() for model in self.secondary_models] + [self.primary_model.adaptive_dropout.get_current_rate()]\n",
        "\n",
        "def generate_segmented_time_series(n_samples, seq_length, n_features, change_points):\n",
        "    total_length = n_samples + seq_length\n",
        "    time = np.arange(total_length) / 100.0\n",
        "    series = []\n",
        "\n",
        "    for i in range(n_features):\n",
        "        feature = np.zeros(total_length)\n",
        "        for j in range(len(change_points) + 1):\n",
        "            start = 0 if j == 0 else change_points[j-1]\n",
        "            end = total_length if j == len(change_points) else change_points[j]\n",
        "\n",
        "            if j % 4 == 0:  # Linear trend\n",
        "                feature[start:end] = 0.05 * time[start:end] + 0.1 * i\n",
        "            elif j % 4 == 1:  # Sine wave\n",
        "                feature[start:end] = np.sin(2 * np.pi * 0.05 * time[start:end])\n",
        "            elif j % 4 == 2:  # Increased noise\n",
        "                feature[start:end] = np.random.normal(0, 0.5, size=end-start)\n",
        "            else:  # Complex sine\n",
        "                feature[start:end] = (\n",
        "                    0.5 * np.sin(2 * np.pi * 0.03 * time[start:end]) +\n",
        "                    0.3 * np.sin(2 * np.pi * 0.07 * time[start:end])\n",
        "                )\n",
        "\n",
        "            # Add some noise to all regimes\n",
        "            feature[start:end] += np.random.normal(0, 0.05, size=end-start)\n",
        "\n",
        "        series.append(feature)\n",
        "\n",
        "    return np.array(series).T\n",
        "\n",
        "# Set hyperparameters\n",
        "input_size = 5\n",
        "hidden_size = 64\n",
        "output_size = input_size\n",
        "num_secondary = 3\n",
        "delays = [1, 5, 3]\n",
        "secondary_model_types = ['lstm', 'gru', 'lstm']\n",
        "primary_model_type = 'lstm'\n",
        "seq_length = 50\n",
        "n_samples = 1000\n",
        "batch_size = 32\n",
        "num_epochs = 60\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Dropout parameters\n",
        "dropout_params = {\n",
        "    'initial_rate': 0.25,\n",
        "    'min_rate': 0.1,\n",
        "    'max_rate': 0.9,\n",
        "    'adaptation_rate': 0.001\n",
        "}\n",
        "\n",
        "# Define change points for the features\n",
        "change_points = [250, 500, 750]  # Example change points\n",
        "\n",
        "# Generate data\n",
        "data = generate_segmented_time_series(n_samples + seq_length, seq_length, input_size, change_points)\n",
        "\n",
        "# Prepare input sequences and target values\n",
        "X = np.array([data[i:i+seq_length] for i in range(n_samples)])\n",
        "y = data[seq_length:seq_length+n_samples]\n",
        "\n",
        "X = torch.FloatTensor(X)\n",
        "y = torch.FloatTensor(y)\n",
        "\n",
        "# Split data into train and test sets\n",
        "train_size = int(0.8 * len(X))\n",
        "X_train, X_test = X[:train_size], X[train_size:]\n",
        "y_train, y_test = y[:train_size], y[train_size:]\n",
        "\n",
        "# Create data loaders\n",
        "train_dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
        "test_dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize model, loss function, and optimizer\n",
        "model = EnhancedSwappableModelEnsemble(input_size, hidden_size, output_size, num_secondary, delays, secondary_model_types, primary_model_type, dropout_params)\n",
        "criterion = nn.MSELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "model.train()\n",
        "losses = []\n",
        "dropout_rates_history = []\n",
        "for epoch in range(num_epochs):\n",
        "    total_loss = 0\n",
        "    for batch_X, batch_y in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs, attention_weights = model(batch_X)\n",
        "        loss = criterion(outputs, batch_y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    losses.append(avg_loss)\n",
        "    dropout_rates_history.append(model.get_dropout_rates())\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}')\n",
        "\n",
        "# Evaluate the model on the entire dataset\n",
        "model.eval()\n",
        "all_predictions = []\n",
        "all_actuals = []\n",
        "attention_weights_list = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for i in range(len(X)):\n",
        "        input_seq = X[i].unsqueeze(0)  # Add batch dimension\n",
        "        target = y[i].unsqueeze(0)\n",
        "        output, attention_weights = model(input_seq)\n",
        "        all_predictions.append(output.squeeze().numpy())\n",
        "        all_actuals.append(target.squeeze().numpy())\n",
        "        attention_weights_list.append(attention_weights.squeeze().numpy())\n",
        "\n",
        "all_predictions = np.array(all_predictions)\n",
        "all_actuals = np.array(all_actuals)\n",
        "attention_weights = np.array(attention_weights_list)\n",
        "\n",
        "# Visualize the full time series with predictions\n",
        "plt.figure(figsize=(20, 15))\n",
        "for i in range(input_size):\n",
        "    plt.subplot(input_size, 1, i+1)\n",
        "\n",
        "    # Plot full data\n",
        "    plt.plot(data[seq_length:, i], label='Full Data', alpha=0.5)\n",
        "\n",
        "    # Plot actual and predicted values for the entire series\n",
        "    plt.plot(all_actuals[:, i], label='Actual', linewidth=2)\n",
        "    plt.plot(all_predictions[:, i], label='Predicted', linewidth=2)\n",
        "\n",
        "    plt.title(f'Feature {i+1}')\n",
        "    plt.legend()\n",
        "\n",
        "    # Add vertical lines for change points\n",
        "    for change_point in change_points:\n",
        "        if change_point >= seq_length:\n",
        "            plt.axvline(x=change_point - seq_length, color='r', linestyle='--')\n",
        "\n",
        "    # Add labels for each segment\n",
        "    for j, change_point in enumerate(change_points + [len(data)]):\n",
        "        if change_point >= seq_length:\n",
        "            prev_change_point = seq_length if j == 0 else max(change_points[j-1], seq_length)\n",
        "            mid_point = (prev_change_point + change_point - seq_length) // 2\n",
        "            segment_type = ['Linear', 'Sine', 'Noise', 'Complex Sine'][j % 4]\n",
        "            plt.text(mid_point, plt.ylim()[1], segment_type, horizontalalignment='center', verticalalignment='bottom')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Calculate and print the Mean Squared Error for each feature\n",
        "for i in range(output_size):\n",
        "    mse = np.mean((actuals[:, i] - predictions[:, i])**2)\n",
        "\n",
        "# Visualize attention weights\n",
        "plt.figure(figsize=(10, 5))\n",
        "mean_attention = attention_weights.mean(axis=2)\n",
        "mean_attention_list = [mean_attention[:, i, 0] for i in range(mean_attention.shape[1])]\n",
        "plt.boxplot(mean_attention_list)\n",
        "plt.title('Distribution of Mean Attention Weights for Each Secondary Model')\n",
        "plt.xlabel('Secondary Model (by delay)')\n",
        "plt.ylabel('Mean Attention Weight')\n",
        "plt.xticks(range(1, len(delays) + 1), delays)\n",
        "plt.show()\n",
        "\n",
        "# Print average attention weights\n",
        "avg_attention = mean_attention.mean(axis=0)\n",
        "for i, delay in enumerate(delays):\n",
        "    print(f\"Average attention weight for model with delay {delay}: {avg_attention[i][0]:.4f}\")\n",
        "\n",
        "# Visualize attention weights over time\n",
        "plt.figure(figsize=(20, 5))\n",
        "for i in range(len(delays)):\n",
        "    plt.subplot(1, len(delays), i+1)\n",
        "    plt.imshow(attention_weights[:, i, :].squeeze().T, aspect='auto', cmap='viridis')\n",
        "    plt.title(f'Attention Weights for {secondary_model_types[i].upper()} (Delay {delays[i]})')\n",
        "    plt.xlabel('Sample')\n",
        "    plt.ylabel('Time Step')\n",
        "    plt.colorbar()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Visualize dropout rates during training\n",
        "plt.figure(figsize=(12, 6))\n",
        "dropout_rates_history = np.array(dropout_rates_history)\n",
        "for i in range(dropout_rates_history.shape[1] - 1):\n",
        "    plt.plot(dropout_rates_history[:, i], label=f'{secondary_model_types[i].upper()} (Delay {delays[i]})')\n",
        "plt.plot(dropout_rates_history[:, -1], label=f'Primary {primary_model_type.upper()}', linestyle='--')\n",
        "plt.title('Dropout Rates During Training for Each Model')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Dropout Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Visualize dropout rate vs attention weight\n",
        "plt.figure(figsize=(20, 5))\n",
        "for i in range(len(delays)):\n",
        "    plt.subplot(1, len(delays), i+1)\n",
        "    # Use mean dropout rate per batch for plotting\n",
        "    mean_dropout_rates = dropout_rates_test[:, i].mean()\n",
        "    plt.scatter(mean_attention[:, i, 0], np.full_like(mean_attention[:, i, 0], mean_dropout_rates), alpha=0.5)\n",
        "    plt.title(f'Dropout Rate vs Attention Weight\\n{secondary_model_types[i].upper()} (Delay {delays[i]})')\n",
        "    plt.xlabel('Mean Attention Weight')\n",
        "    plt.ylabel('Dropout Rate')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print final dropout rates\n",
        "final_dropout_rates = model.get_dropout_rates()\n",
        "for i, rate in enumerate(final_dropout_rates):\n",
        "    print(f\"Final dropout rate for {'Primary' if i == len(final_dropout_rates) - 1 else 'Secondary'} Model {i+1}: {rate:.4f}\")"
      ]
    }
  ]
}